[
  {
    "path": "posts/2024-01-15-estimating-epidemiological-delay-distributions-for-infectious-diseases/",
    "title": "Estimating epidemiological delay distributions for infectious diseases",
    "description": "I summarise our research on refining delay distribution estimates in epidemic modeling, a journey prompted by my general confusion. We explore and compare various approaches, drawing insights from simulations and Ebola virus disease outbreak data, and suggest paths for future improvements. Also featuring Poppy, the puppy, who is currently gnawing on my hand and occasionally archiving my emails.",
    "author": [
      {
        "name": "Sam Abbott",
        "url": "https://samabbott.co.uk"
      }
    ],
    "date": "2024-01-15",
    "categories": [],
    "contents": "\n\nContents\nWhere can I read it?\nWhat motivated this work?\nWhat is the paper about?\nFuture work\nPoppy the puppy\n\nNote this was written at 2am whilst sitting up with Poppy the puppy who was dealing with the trauma of her first outside the house walk.\nPoppy the puppy\nWarning: One five year-old described this paper as double extra boring - these comments have been noted and we are working on it (we still think it is both interesting and important though).\n\nWhere can I read it?\nYou can read the paper here. If interested in the code repository you can check it out here. The in-progress package that generalises the paper code is available here (contributions welcome!).\nWhat motivated this work?\nMy involvement in this work grew out of my general confusion about how to best estimate delay distributions during outbreaks, and in particular, how the various methods that are used in practice compare and interact. I have been grappling with this problem for a while, but never to my complete satisfaction. This all came to a head when I was asked to help write an editorial for Ward et al. 2022, and I realized I needed some help.\nAs is often the case when deep thinking is required, I reached out to Sang Woo Park for assistance. This began a rather long conversation where my main contribution was repeatedly admitting, “I don’t understand.” After a while, we arrived at this gist, which finally made things click for me.\nIn the course of this back-and-forth, we read a lot of the related literature and realized that there was generally a lot of confusion about how to estimate delay distributions and how to compare methods. We also recognized that there was significant confusion about the various biases that can be introduced and how they interact. Obviously, this was a substantial topic, so we needed the help of some very tolerant and talented co-authors to make a dent in it. Time to light the beacons and call for help!\nThis collaboration led to a very enjoyable and productive period of tackling some of the challenges in estimating delay distributions. I am deeply grateful to all the co-authors for their patience and hard work, especially given the long gestation period of this paper due to its complexity and it being largely a side project for all involved. I’m also looking forward to continuing to work on this topic and hopefully making progress on the many open questions that remain.\nWhat is the paper about?\nBackground\nImportance of accurate distribution estimates: Key for epidemic modeling and decision-making.\nRight truncation: Overlooking this leads to underestimated delay during real-time analysis.\nInterval censoring: When both events that make up a delay are only known down to an interval, double censoring needs attention.\nDynamical bias: Similar to demographic changes, affecting observed delay distributions.\nExisting methods: Methods exist but lack systematic comparison or full bias adjustment.\nWhat exactly did we do?\nTheory overview: Clarifies the theory behind estimating distributions.\nReview of methods: Examines various methods in light of the theory we have discussed.\nMethod evaluation: Utilizes simulations and data from the Ebola epidemic for method comparison and evaluation.\nGuidance for practice: Offers cautionary and practical guidance for estimating delays, including future research directions. (Kelly Charniga is leading on a follow-up piece that will be more focused on this as well as offering advice on best practice more generally.).\nKey Findings\nImpact of neglecting biases: Ignoring biases leads to flawed epidemiological estimates in nearly all scenarios.\nDynamical bias and right truncation: In a growing epidemic, these biases are nearly equivalent for delays with independent events and can be adjusted for using the same methods (TLDR: Don’t adjust for both in these settings ).\nCommon censoring adjustment issues: Naive discretization can cause biased estimates. Many ad-hoc methods exist and often perform poorly.\nMethod performance: Highlights the best performing methods (generally treating primary and secondary events as latent variables and estimating a continuous distribution normalised by the probability of observing the secondary event worked well. See the paper - its simpler than it sounds!) and their limitations.\nResidual biases: Persist across all methods, especially due to uniform interval distribution assumptions when the delay was short compared to the censoring interval. This is an area of further work!\nRead the paper for more details (there is a summary at the beginning and in every section to get you going - we know it is long)!\nFuture work\nWe currently talk about an exact method but don’t evaluate it. In the next version we will do so (early signs show this approach may be impractical for real-world usage as it stands).\nWe are working on a follow-up piece that will be more focused on practical guidance for estimating delays, including future research directions.\nWe are working on a package that implements the methods disscussed in the paper. As all the models in this work are extensions of {brms} all of these model can support arbitrary strata and time-varying components - how neat!\nMore evaluation of time-varying delays (as above our models support these but we didn’t have time to evaluate them in this paper).\nExploration of methods for estimating the generation time distribution (the distribution of the time between infection of a primary case and infection of a secondary case) - this is a related but distinct problem.\nPoppy the puppy\nIf I entrapped you with cute puppy pictures then I apologise. Here is another picture of Poppy the puppy to make up for it. She is a 13 week old Lab/Border Collie mix from near Stonehenge. She is very smart, cute, and a serial killer (so we get on well).\nPoppy the puppy just being totally normal and not planning a sustained campaign of guerrilla insurgency\n\n\n",
    "preview": "posts/2024-01-15-estimating-epidemiological-delay-distributions-for-infectious-diseases/poppy.png",
    "last_modified": "2025-03-18T11:09:01+00:00",
    "input_file": {},
    "preview_width": 1529,
    "preview_height": 1487
  },
  {
    "path": "posts/2023-01-04-iisa-data-models-and-modellers-during-a-global-pandemic/",
    "title": "Data, Models, and Modellers during a Global Pandemic",
    "description": "My notes for my points and answers as part of the Data, Models, and Modellers during a Global Pandemic discussion session at the International Indian Statistical Association 2022 meeting. Also includes the recording of my talk on our recent paper \"Evaluating an epidemiologically motivated surrogate model of a multi-model ensemble\" as part of the Pandemic forecasting: Lessons learnt from COVID-19 session.",
    "author": [
      {
        "name": "Sam Abbott",
        "url": "https://samabbott.co.uk"
      }
    ],
    "date": "2023-01-04",
    "categories": [],
    "contents": "\n\nContents\nPandemic forecasting: Lessons learnt from COVID-19 session session\nData, Models, and Modellers during a Global Pandemic\nIntroduction\nMe\nData\nModels\nModellers\nQuestions\n\n\nAs my last work commitment of 2022 I took part in a discussion session with Swapnil Mishra and Rukmini S. (whose book I am looking forward to reading) on Data, Models, and Modellers during a Global Pandemic discussion. I thought some interesting points were made but sadly, as far as I am aware, there is no recording. As a very poor substitute to what was an interesting discussion I have included the notes for my points and answers below. I aimed to give an overview of how the key topics (data, models, and modellers) changed over the last three years for me.\nI also gave a talk on our recent paper “Evaluating an epidemiologically motivated surrogate model of a multi-model ensemble” and I’ve included the recording below. As ever, I sound quite insane to myself when listening back but hopefully you can push past that as I think the paper is interesting and worth a read.\nThanks to Bhramar Mukherjee for the invitation and for hosting the discussion so ably. Apologies for my terrible organisational abilities - I promise it isn’t just a technique to make myself appear like an academic stereotype (perhaps it would be better if it were).\nIf anyone listening to this thought “Oh my what an engaging chap. I wish we could have him to talk” - The answer is yes you can! Just give me a ping (and then maybe another one a short time later).\nPandemic forecasting: Lessons learnt from COVID-19 session session\n\n\nSlides for this talk are available here.\nYou can read the paper here.\nIf interested in the code repository you can check it out here.\nData, Models, and Modellers during a Global Pandemic\nIntroduction\nFirstly thank you very much for inviting me to this panel I am looking forward to the discussion.\nBrief intro to me and my work.\nA brief overview of my experience of each of the main areas of this panel over the course of the pandemic and in particular highlight how this has changed with time.\nMe\nBackground in mathematics and theoretical physics (this was bit hard for me). Moved into mathematical biology\nPhD in Bristol. Infectious disease modelling. BCG for Tuberculosis\nA short trip into finance/fintech\nStarted at LSHTM on January the 6th as part of a team to learn about modelling real-time infectious disease, focussed on Cholera.\nSwitch to COVID on the 12th of January.\nEarly work estimating the reproduction number and the feasibility of contact tracing.\nMoved to estimation of the reproduction number and short term forecasting both for UK stakeholders and globally on a daily basis for 4k+ locations (our platform ended the pandemic with > 1 million unique users).\nAll work developed as open source software packages and now widely used by public health departments, WHO, CDC, ECDC for a range of diseases (for example by the CDC for the recent Monkeypox outbreak).\nFor the last 3 years kept iterating on this type of work along with reactive analysis (i.e transmissibility of variants etc) and more traditional academic (most recently particularly focussed on nowcasting methodology). Now shifting to writing this up for more traditional academic output.\nMain aim is to democratise access to robust methodology and generally improve the standard of real-time work which historically is often of lower quality than more traditional retrospective work.\nData\n2020\nI started work on COVID-19 on the 12th of January. At this time we still only had case data coming out of China. This was sparsely reported and often delayed.\nReports of detected cases in other countries started coming in and we started to use these both to understand the outbreak in China and to try and get a handle of local transmission.\nData became commonly available by day of test and through official sources.\nWe set up our own data collection methodology (covidregionaldata) to service our modelling pipeline as no open access/open methodology sources were available.\nAs the outbreak progressed to be a pandemic UK data sources began to come online with line list data (though this was often very incomplete). Government data with reduced delays and more granularity also started being provided to us.\nAs testing became more widespread surveillance moved from hospitalised cases to community test positive cases (though in reality we used a blend of routine data)\nData from other countries (at least at the national level) became easier to source but we still relied on our own framework for sub-national data.\n2021\nAppreciation of the unreliability of much testing data.\nPivot back to the use of hospitalisations in the UK.\nUse of excess deaths data to highlight the global impact of the pandemic.\nIncreased reliance on novel data sources like the ONS infection survey.\nMore use (by us) of sequence data sources built by community contributions (like GISAID).\nDiscussion of novel data sources (like cycle threshold by James Hay).\n2022\nTesting data less likely to be available daily.\nTesting data increasingly unreliable and other surveillance sources increasingly sparsely reported.\nIncreased reliance on seroservalance, household studies, and prevalence studies.\nShift in focus to working with Public health departments on their very rich data sources.\nModels\n2020\nAvailable methods and software not useful for the vast majority of real-world questions we faced. Largely did not make use of epi domain tooling (but general statistical tooling was very useful).\nReactive simple models. Designed based around available data sources and particular short-term analysis questions.\nOver time these developed into several routine “pipelines” (i.e chains of models) that we used to produce estimates of interest both in the UK for stakeholders and publicly world wide.\nIn mid-2020 colleagues alerted us to issues (Katie Gostic) with our pipeline approach and we pivoted to a generative Bayesian model for our routine work.\n2021\nThe detection and spread of new variants (of course this is also late 2020) could not be easily modelled (i.e not in the timeline required by policy makers) using our routine methods.\nWe (and many colleagues) therefore had to go back to the model pipeline approach feeding output from one model into another as a data source (i.e to estimate variant transmissibility).\nOver time many specialist models were developed to deal with this particular question (both by us and others).\nMore generally we began to develop more specialised models for particular high quality data sources (such as the ONS infection survey).\n2022\nThe spread of Omicron posed challenges for these models in particular the potential for immunity evasion and a shorter generation time.\nOf those specialised models developed in 2021 that had not been abandoned/become inoperable most could not account for these features.\nEarly analysis (by us and others) therefore again fell back on the pipeline approach of using model outputs as data in order to evaluate the evidence for a different generation time and immune escape.\nMore detailed statistical modelling approaches to work with the detailed linelist data from partners (i.e nowcasting, Cycle threshold modelling). This seems to be a better pathway to impacting decision making.\nModellers\n2020\nHuge community effort with many people from other fields.\nI personally worked ~ 18 hours a day for about 6 months.\nOften hard to collaborate on work especially across knowledge silos. This led to lots of poorly linked efforts and much repetition.\n2021\nMany colleagues - especially from other fields pivoted away from COVID-19.\nThis meant that in many ways the amount of work to do actually increased.\nWe received very little support for our routine modelling.\nAs modelling became more complex we had to fall back on simpler methods due to lack of resources.\n2022\nShift in focus to learning lessons from the pandemic and getting ready for the future.\nTrying to build a community of practice as community >> methods or models alone.\nAs we move into 2023 hitting pause a little to take a breath and identify what the key challenges are and how the system can be made to recognise and respond to them.\nQuestions\nHow did the data science work during the pandemic influence your post-pandemic academic scholarly work\nProfoundly. I would say I am borderline no longer now an academic in the traditional sense and I am not sure I am interested in moving back in this direction.\nVery interested in thinking about how to develop a community of practice and a community supported suite of outbreak surveillance tools. Also interested in why efforts in this direction keep failing.\nLessons learnt about when it is useful to have a custom method and when we need more custom methodology was very important. In general I now try to make all work as modular as possible and as easy for others to hack on as possible.\nThe other vital realisation is how important being at least 80% but on time was (there is a nice paper by Chris Whitty (UK chief medical officer) on this). We need methods that allows us to do this for as many questions as we are likely to have in a similar scale outbreak (which could look very different in ways that are hard to predict).\nWhat do you think are the biggest takeaways in terms of Scientific communications with the public and stakeholders? Some do’s and some don’ts\nBe as open as possible.\nAlways clearly state the limitations of your work (ideally first).\nNo point estimates!\nTechnical detail needs to be present but often not in the main report (just list the main assumptions and limitations in plain text).\nJoint statements across teams are very useful for public facing work as generally there is a broad consensus with different views on the details. If results are presented individually this can get lost and it incentivises being first out (and maybe therefore cutting corners). We saw a lot of rushed work during the pandemic because of poorly aligned incentive structures.\nI personally didn’t feel it was useful for me to directly interact with the public/media. No training. No time. Explaining the evidence not my speciality leave it to others for whom that is a speciality.\nHow do we reward such contributions in more traditional academic setting?\nThis is really a question for those making these decisions. I have yet to see much movement on this which is a shame and it is nearly too late for those who worked on the pandemic.\nLack of leadership here from senior academics and funders who have done well in the current system.\nThere was a large amount of funding in the some locations for pandemic adjacent work but most of this was assigned using very traditional approaches.\nI think a good start would be valuing outputs other than peer reviewed papers more (like software and support of public health departments etc). There is pushback on this because it is “hard” but ultimately if you want to incentivise better work then more effort is needed by those allocating resources.\nWhat is your next project\n{epinowcast}: Aiming to deal with some of the issues I outlined above and provide a consistent set of tooling to answer outbreak and surveillance questions in real-time without having to reduce the quality of the work to below the 80% target threshold.\nIs the pandemic over\nFor me yes but not globally.\n\n\n\n",
    "preview": "posts/2023-01-04-iisa-data-models-and-modellers-during-a-global-pandemic/lead-slide.png",
    "last_modified": "2025-03-18T11:09:00+00:00",
    "input_file": {},
    "preview_width": 5080,
    "preview_height": 2857
  },
  {
    "path": "posts/2022-10-13-how-to-prepare-poll/",
    "title": "How should we build methods and tools to prepare for future pandemics?",
    "description": "There are several models that we could take to develop methods and tools in order to prepare for future pandemics. We should talk about them.",
    "author": [
      {
        "name": "Sam Abbott",
        "url": "https://samabbott.co.uk"
      }
    ],
    "date": "2022-10-13",
    "categories": [],
    "contents": "\nPandemic preparedness has been in vogue for a while (for obvious reasons)1. The majority of the conversation I’ve seen has been on how to get more, and longer term, funding and on specific initiatives we should focus on2. I have seen much less discussion about how we go about doing this work, whether our traditional research models are a good fit, or if something more is needed. When this is discussed the impression I get is that if traditional research is not the way to go then new organisations, that have provider/user models, are favoured but to me this seems like a potentially very risky way to go about things3.\nOver the last several years I have been working on real-time infectious disease modelling4 with the aim of providing evidence faster, and better, than was previously possible. In the main this has proved very very hard, as I know many others have found. In many cases the major issue has not been technical ability (though for me at least perhaps this is a limitation5), or a lack of understanding of what the problem is, but rather the sheer scope of the task. This is compounded by the lack of resources that are connected to the appropriate communities of practice. This makse it hard to even start answering some of the more pressing questions, or in some cases even to understand what they are, in the amount of time available. This has been the case for both reactive work but also preparedness work where the timelines are longer (a few years) but the resources still seem insufficient for the task in hand.\nFor a while I have been convinced that a community approach6 is the right way to go about this. What do I mean by this? I mean creating something where people (and ideally users) can contribute to a large project - or body of knowledge - whilst retaining ownership, and the credit that comes with it, of their contributions. On the face of it this is how science already works but in practice I think contrasts with the traditional model where separate groups/organisations create competing tools and methodologies and then in theory (but often not in practice) provide these as a service to others7. This is a hard sell but quite a few practioners appear to agree.\nThis could all just be my echo chamber and there are a wide range of views8. For a while I have been running Twitter polls9 to scope out what people think. I recently came across pol.is which is a platform designed for these sort of conversations. From a first pass it seems great. Below is a conversation I’ve started aiming to find out what people think. Currently the questions are a pretty eclectic mix that I put together but you can and should submit your own. Initial results are shaping up to be pretty interesting.\nIf you find this useful/interesting please consider sharing it in your network. I think it’s important we think more about how we do work before attention from this problem fades yet again.\n\n\n\n\nThere is a lot to this but here I just focus in on my domain interest which is processing data to get useful signals to inform decisions.↩︎\nFor example certain groups are very in faviour of metagenomic scanning and others want to develop a weather forecasting service for infectious disease.↩︎\nBorne out I think by the recent shuttering of the Pandemic Preparedness Institute: https://www.science.org/content/article/cycles-panic-and-neglect-head-pandemic-prevention-institute-explains-its-early-death↩︎\nA very biased round-up for a (sadly unsuccessful) promotion application: https://samabbott.co.uk/posts/2022-04-11-a-very-biased-view-of-my-recent-research/↩︎\nSome thoughts on limitations with my work here: https://samabbott.co.uk/posts/2022-09-19-80-percent-good-enough/↩︎\nSee this starting to take shape here: https://www.epinowcast.org/↩︎\nSound good? Get involved: https://community.epinowcast.org↩︎\nClearly given the numerous projects starting to take shape based on similar ideas but with very different approaches.↩︎\nI am a very serious scientist.↩︎\n",
    "preview": "posts/2022-10-13-how-to-prepare-poll/poll.png",
    "last_modified": "2025-03-18T11:09:00+00:00",
    "input_file": {},
    "preview_width": 704,
    "preview_height": 446
  },
  {
    "path": "posts/2022-10-13-surrogate-ensemble-forecast/",
    "title": "Evaluating an epidemiologically motivated surrogate model of a multi-model ensemble",
    "description": "Multi-model and multi-team ensemble forecasts have become widely used to generate reliable short-term forecasts. Whilst there is robust and consistent evidence that multi-team ensemble forecasts provide reliable and performant forecasts across domains they also have a range of downsides. The most significant is the difficulty in interpreting them. Here we develop a parsimonious forecast model based on observations of ensemble behaviour and evaluate its predictive performance.",
    "author": [
      {
        "name": "Sam Abbott",
        "url": "https://samabbott.co.uk"
      }
    ],
    "date": "2022-10-13",
    "categories": [],
    "contents": "\n\nContents\nWhere can I read it?\nWhat we did\nWhy does this paper exist?\n\nLight the beacons - we have a new preprint\nWhere can I read it?\nYou can read the paper here. If interested in the code repository you can check it out here. I particularly recommend the commit messages as they give a fun window into the writing process. For the kind of people that just want model code check it out here.\nWhat we did\nWe discuss observations, advantages, and limitations of the European Forecast Hub and our own prior submissions.\nWe use these observations to develop a simple model meant to replicate the Forecast Hub ensembles behaviour.\nWe compare our surrogate model with the Forecast Hub ensemble over period of 6 months using forecasts submitted each week to the Forecast Hub.\nWe do lots of detailed forecast evaluation and try hard to get some insights.\nWe round it out with a critique of our work and some suggestions for the future.\nWe also provide a GitHub powered workflow for producing a forecast so you can make your own1.\nWhy does this paper exist?\nOver the last few years we have produced a lot of forecasts. In particular, we have submitted weekly to US, European, and German/Poland Forecast Hubs using a range of models for an extended period of time2. An interesting aspect of submitting to these platforms is that your forecast becomes part of another product, the forecast ensemble. This is a good thing as ensembles are typically more robust than forecasts from single models and in many cases give better performing forecasts. However, they can be very hard to learn from, and though we have really tried to learn more about how to forecast it has been difficult.\nIn the summer of 2021, I was getting a little frustrated at the lack of progress improving our forecasting approaches. It felt like we needed to go back to the basics of the models we were using in order to learn things. I was also seeing a lot of discussion about the use of genomic data for surveillance and wanted to do some investigation. As great minds think alike, Johannes Bracher had recently developed a short-term forecasting model that included variant dynamics. I shamelessly stole this3 and reimplemented it in a modern PPL4 - creating the forecast.vocs R package. Everything was steaming ahead to do a large-scale evaluation of using sequences for short-term case forecasts5. Unfortunately, storm clouds were brewing.\nIn the winter of 2021 we started to see news of the Omicron variant with the first reports coming out of South Africa6. This looked like bad news. Several of the authors of this study helped pivot the forecast.vocs R package to start looking at the transmission advantage of this variant, as well as trying to estimate if it was changing over time (as an indicator of immune escape), and to produce short-term case forecasts7. We were able to do this as, unlike forecast ensembles, our model was very simple and easy to modify.\nSadly, I like many people then proceeded to catch COVID-198. This meant I was consigned to the house with really very little to do whilst cooling down from a very intense work period9. The natural thing to do was browse the various Forecast Hub websites and think about how the models were performing and trying to see if they were managing to deal with the rise of Omicron. After jotting down some notes about how I thought things were working I realised that the model we had been using for our Omicron work was perhaps a good test bench for seeing if my ideas for why the ensemble behaved the way it did were correct. Some intense hacking on the forecast.vocs model followed and this paper was born.\nAfter this semi-delusional rampage some adults10 entered the picture and transformed this from a vague idea into a complete bit of work. It has been really great working with them all and I would thoroughly recommend it.\n\nIf interested in this you can use this to submit your forecasts to the European Forecast Hub: https://github.com/covid19-forecast-hub-europe/covid19-forecast-hub-europe-submissions↩︎\nYou can read more about some of these methods here: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010405↩︎\nwith some adaption perhaps↩︎\nshots fired JAGs↩︎\nWe are now circling back to this and I think it should turn out to be pretty interesting. You can check out what is already in place and keep and eye out for more here: https://github.com/epiforecasts/evaluate-delta-for-forecasting↩︎\nGreat job SA!↩︎\nSee a version of the report here: https://github.com/epiforecasts/omicron-sgtf-forecast↩︎\nOn Christmas day from an extended family member - it was a little awkward.↩︎\nDaily model updates and a several other related bits of work - tiring!↩︎\ni.e. my lovely co-authors↩︎\n",
    "preview": "posts/2022-10-13-surrogate-ensemble-forecast/forecast-performance.png",
    "last_modified": "2025-03-18T11:09:00+00:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 1536
  },
  {
    "path": "posts/2022-09-19-80-percent-good-enough/",
    "title": "What is 80% good enough for real-time infectious disease analyses?",
    "description": "A comment piece exploring what factors led to limitations in some of the work we produced during the COVID-19 pandemic, whether this work was good enough, and some potential ideas for improving future work of a similar kind.",
    "author": [
      {
        "name": "Sam Abbott",
        "url": "https://samabbott.co.uk"
      },
      {
        "name": "Joel Hellewell",
        "url": "https://jhellewell14.github.io/"
      }
    ],
    "date": "2022-09-19",
    "categories": [],
    "contents": "\n\nContents\nSummary\nIntroduction\nCase studies\nConclusions\nAcknowledgements\n\nSummary\nThe COVID-19 pandemic has seen an unprecedented use of infectious disease research conducted over short time scales as a tool for setting health policy. However, it has been widely recognised that some of this work was flawed. These flaws were often acknowledged by the authors but in some cases were not. Here we explore some of the underlying reasons for these limitations using examples drawn from our own work. We do not highlight limitations in the work of others but expect these would be similar. This means that decisions made using a wide array of sources of evidence may still be vulnerable to these issues. We welcome, and encourage, similar reflections from other groups contextualised using their work.\nWe find that key issues include: lack of technical and statistical knowledge, poor availability of domain-specific tools, limited evaluation of available methods and tools, difficulty in maintaining long-term projects, and lack of support for gradual advance outside periods of crisis. We conclude that researchers should be more open and realistic about the limitations of their work and the reasons for them and that more focus and support should be given to evaluation, incremental improvement, and the development of domain-specific tools if we wish to improve the quality of evidence available in real-time for future outbreaks.\nThis piece was originally written in August 2021 and shared with colleagues, but as we see it little progress has been made since on the issues we raise. The recent Monkeypox outbreak and response highlights this lack of progress and the limited lessons learned so far from the COVID-19 pandemic.\nIn February 2022, SA gave a talk at the LSHTM symposium How can mathematical and statistical models combine with big data to improve our response to pandemics (S. C. Whitty and McLean, n.d.) which touched on some of the same issues and for which a recording is available1. SA, along with Sebastian Funk, has also written a related reflection on our efforts to produce real-time effective reproduction estimates (Abbott and Funk 2022). Since writing this piece we have also become involved in the Epinowcast community project (“Welcome to the Epinowcast Community,” n.d.) which aims to deal with many of the issues we raise here. If interested in supporting this project either via contributions, community participation, or financial support please reach out.\nIntroduction\nA great deal of the scientific response to the COVID-19 pandemic involved estimating as quickly as possible the most pertinent biological and epidemiological properties of the novel respiratory virus SARS-CoV-2. This research often fed into government policy decisions related to responding to the outbreak, and therefore needed to be as accurate as possible. However, there is an inevitable trade-off between speed and accuracy in scientific research, as developing a thorough and nuanced analysis for which you are confident of its accuracy takes time and resources. Scientists are required to judge when their work has reached a level of accuracy that is “good enough” to be shared with policymakers or advisory panels so that a decision can be made. Working longer would often increase the accuracy of the work but it may not add further information that would aid in decision-making. This attitude is reflected in this quote from Chris Witty, the UK Government Chief Medical Adviser (C. J. M. Whitty 2015):\n\nAn 80% right paper before a policy decision is made is worth ten 95% right papers afterwards, provided the methodological limitations imposed by doing it fast are made clear.\n\nThis quote, as well as the attitude it represents, is commonly referenced in much of the academic discourse (Brooks-Pollock et al. 2021) around real-time analyses conducted to inform COVID-19 health policy. Less attention is commonly given to what is considered 80% right and the conditional statement that limitations must be clearly stated. Here, using examples from our policy adjacent COVID-19 work, we explore what it means for work to be 80% good enough, and highlight issues that prevent it from being better or limitations being more clearly stated.\nCase studies\nEffective reproduction number estimation\nThe reproduction number has been a key epidemiological measurement during the COVID-19 pandemic. Estimating the basic reproduction number from early data emerging from Wuhan province, China, indicated the infectiousness, and pandemic potential, of the then-unknown novel pathogen (Abbott, Hellewell, Munday, et al. 2020). The effective reproduction number has also been extensively used to track the course of the pandemic and evaluate the impact of interventions in many countries. We started estimating the effective reproduction number in real-time in February 2020, with a focus on China before expanding to include national estimates globally, as well as estimates at smaller scales in multiple countries and with a range of data sources (Abbott, Hellewell, Thompson, et al. 2020a, 2020b). Our estimates for the United Kingdom fed into the Scientific Pandemic Influenza Group on Modelling (SPI-M) aggregated reproduction number estimates and short-term forecasts (Sherratt et al. 2021; Funk et al. 2020) and our site presenting these estimates had over 500,000 unique users including public health decision makers (Abbott and Funk 2022). Estimates were produced daily for many different countries across multiple geographic aggregations and surveillance data sources. The open-source tools we developed as part of this project have also been used widely by other research groups and public health practitioners for a range of real-time analyses (Abbott, Hellewell, Sherratt, et al. 2020).\nAs these estimates were produced in an automated fashion on a routine basis, at scale across highly varied settings, and by a resource-constrained research group they had many limitations. In our published work, we reported a range of these limitations including the lack of location-specific data on the time from infection to the case report, lack of clarity on the interaction between the generation time and the reproduction number, difficulty in extrapolating current reported cases to infer the dynamics of current infections, and the general assumption that aside from the effective reproduction number other parameters were static in time.\nHowever, our initial work also included an unknown, and hence unreported, limitation in that it assumed that the distribution of the time delay from infection to report could be treated as reversible (Abbott, Hellewell, Thompson, et al. 2020a). This has since been investigated and has been shown to result in over-smoothed, biased estimates that lag behind real-world changes in transmission (Gostic et al. 2020). Though this issue has since been mitigated via methodological developments (Abbott, Hellewell, Thompson, et al. 2020b) it is likely that it resulted in flawed inferences. The primary reason for our initial flawed implementation was our lack of familiarity with the relevant literature for real-time reproduction number estimation and incidence curve reconstruction, compounded by the general conflation of real-time and retrospective methodologies in the public health community.\nMethods to estimate infections from the reported cases were widely used during the HIV/AIDS epidemic (Gostic et al. 2020), but the literature available at the start of the pandemic was, as far as we are aware, largely historical and had seen limited continued development. Whilst tools existed to estimate the effective reproduction number targeting real-time settings these had in general not been developed with the ability to handle the realities of delayed reporting and time-varying ascertainment (Cori et al. 2021). Other tools and methods had been explored to deal with reporting delays (Salmon, Schumacher, and Höhle 2016; Meyer, Held, and Höhle 2017) but in general, these had not been fully evaluated, were often not robust to real-time data challenges, and were difficult to link together to answer real-time questions of interest. Much research work has been done for specific settings but in general, this rarely appears to lead to generalisable methods and in many cases, it is difficult to verify how well the approaches work outside the context for which they were developed. This limitation was a result of a lack of knowledge within our research group but unacknowledged limitations of this kind will remain unless methodological developments are propagated into robust, and well-evaluated, tooling since much of the real-time analysis was and will continue to be done by early career researchers who may lack the breadth of experience necessary.\nOther key issues with this project were its continuous, repetitive nature, the lack of discrete research outcomes, and its large scope. Due to its scale, it required detailed knowledge of multiple statistical and mechanistic modelling approaches, software engineering skills, and insights into each data source used. Initially, whilst the scope of the was somewhat limited, it was possible to incentivise sufficient contributions. As the project continued the scope widened, more maintenance was required, and attention moved on to other problems, meaning that it became more difficult to source sufficient resources. This was compounded once the initial research paper on the methodology had been published and traditional academic credit was no longer available (Abbott, Hellewell, Thompson, et al. 2020a). This resulted in breakdowns in estimation, lack of innovation in the underlying methodology, limited evaluation, less than optimal software engineering making reuse challenging, and flawed estimates in some geographies where the data had unaccounted for eccentricities.\nIdeally, these problems could have been resolved by building collaborations with those developing new methodologies, making use of estimates, or developing effective reproduction number estimation pipelines of their own. In practice, this was difficult to negotiate despite spending significant time and resources with this aim in mind. A range of real-time (and retrospective methods that can be repurposed for real-time usage) estimation methods now exist. Generally, these methods (including our own) have only been partially evaluated and they are rarely designed with reusability, robustness, or routine usage in mind. The situation now is unfortunately not too different to the start of the pandemic, despite significant apparent progress in methodology being achieved over the past two years. A researcher without previous experience in this area looking to perform some real-time analysis would likely still find it time-consuming to either properly evaluate the breadth of different methods themselves, or develop a robust software implementation of an existing method for which it does not already exist. We note that projects now exist to develop tooling to try and overcome some of these limitations but in general these projects are independent siloed efforts that do not build on the work of others. We feel that this approach is unlikely to lead to success given its repeated failure in the past in similar contexts.\nEstimating the transmissibility advantage of variants of concern\nOur second example is two linked studies that were conducted in December 2020 and June 2021 to estimate the transmissibility advantage of the Alpha and Delta variants of COVID-19 (Davies et al. 2021; Abbott, Kucharski, and Funk 2021). Both of these analyses used the same underlying regression framework to model the contribution to the effective reproduction number from the variant of concern compared to all other variants (with this only being a small part of the analyses in (Davies et al. 2021)). This gives a multiplicative transmission advantage estimate for the variant of concern compared to other circulating variants after adjusting for static and time-varying variables influencing transmission. These analyses were presented to SPI-M with the first, after being aggregated with estimates from other sources, being widely reported in the media, often as only a point estimate.\nThese analyses reported a range of limitations including the use of the mean reproduction number estimate and proportion of cases that had the variant of concern, the use of S-gene target failure as a proxy for variant status, the lack of UK-specific generation time estimates, the lack of explicit modelling of auto-correlation, the use of non-parametric reproduction number estimates as an input rather than a single model, and the assumption that the generation time was unchanged between variants. These issues were likely to have led to spuriously precise results, though the magnitude of this is difficult to quantify. Bias may have been introduced by using the mean estimated reproduction numbers and mean proportion of cases with the variant of concern as this weighs all areas equally. Bias may also have been introduced due to the assumption that the generation time was unchanged between variants. It is unclear to what extent these limitations impact the accuracy of these analyses and the resulting decision that may have been partially based on them.\nThis example highlights a general problem in real-time COVID-19 analysis: the quantification of uncertainty(Zelner et al. 2021), or lack thereof. It is difficult to assess the impact of the limitations on the transmission advantage analysis without replication of these studies with improved methods. The analysis was repeated effectively unchanged when concerns were raised about the Delta variant despite several months having passed in which improvements could have been made and it being entirely predictable that similar methodologies would likely be needed in the future. The reasons for this repetition were lack of capacity, both in terms of researcher time and technical infrastructure such as compute resources. Some small progress has since been made in improving this approach (Abbott and Funk, n.d.) but it has not been possible to prioritise due to other, better incentivised, research questions. Unfortunately, this is a commonly repeated pattern of doing an analysis that you hope is 80% correct but cannot check, due to limited time and resources. Then later you end up doing the same analysis when a similar question re-emerges, without any progress toward knowing if you were 80% correct or not.\nIn this instance, the lack of capacity is in large part due to a deluge of other concerns arising in the time between the two variants emerging. It can often be difficult to justify method evaluation as novel and important work, even though it can require substantially more complex methods than the original analysis. Given the likely development of new variants that would require the same work, not prioritising evaluating the accuracy of this analysis after its first application was an oversight. Other research groups have made progress in this area but effective reproduction number estimation solutions for co-circulating variants that are methodologically robust, well-engineered, thoroughly evaluated, and easily generalisable are not as openly available as needed should another pandemic scale outbreak occur in the short term.\nConclusions\nOur examples have highlighted common themes that prevented us from performing fast, accurate real-time analyses. The demand for real-time analyses during the pandemic has exposed the lack of required knowledge in traditional research groups, even in those theoretically geared towards this area, including limited domain knowledge, statistical knowledge, software engineering skills, and experience putting ad-hoc analyses into routine production. These knowledge gaps are compounded by an additional lack of well-developed, and well-evaluated, software tools designed to be used flexibly for real-world analysis questions. Many of the currently available non-domain specific open-source tools, whilst generally high quality, require a substantial level of user knowledge and for relatively common domain-specific problems can require significant researcher time and expertise to produce useful results. Most of the available domain-specific tools have significant limitations, are not continuously supported, and have not been robustly evaluated in a broad enough range of contexts. Another common theme we identify that further compounds these issues is the difficulty, mainly caused by lack of incentives, in collaborating across groups on rapidly developing problems, and on method and tool development more generally.\nWhilst it is difficult to define what is “good enough” for real-time analyses, we have summarised some of the common causes of the limitations within our work during the COVID-19 pandemic. We contend that real-time analysis, because it needs to be performed quickly and with confidence in the level of accuracy, should be undertaken, structured, and incentivised differently from other academic infectious disease research. A suitable analogy might be that firefighters, upon receiving the report that a fire has begun, do not then proceed by designing and developing their tools to fight the fire. The emergency response to a novel pathogen outbreak requires robust, well-understood, well-documented, and thoroughly evaluated methodologies and tools to perform the tasks that we can foresee will be needed each time such an event occurs. Efforts have been made historically to address some of these concerns, mainly for low-level tooling, but we saw little use of these tools during the COVID-19 pandemic (RECON-R Epidemics Consortium, n.d.). We must learn lessons from these initiatives to make progress in improving the response to future outbreaks.\nWe feel that the issues addressed here that stand in the way of good real-time analysis are due to persistent structural flaws within the academic system(Kucharski, Funk, and Eggo 2020) which, while they affect all research, particularly affect real-time analysis where there is less tolerance for inefficiencies and flawed results. Many of the issues we have highlighted are exacerbated by an academic culture that promotes apparent individual scientific achievement over collaborative work, that does not promote a culture of ongoing technical and statistical training, and indeed does not reward or value these skills in applied researchers beyond early career stages.\nAll of the examples in this letter were published with open-source code and the majority are contained in commonly used open-source software tools. In theory, this should allow other researchers to explore these analyses and mitigate some of the limitations but in practice, incentive structures favor new work rather than incremental improvements and evaluation. We feel it is vital that we and others evaluate real-time analyses, improve on the methodology used, and expand the supply of domain-specific tooling so that future work can be of higher quality. This can most easily be done when researchers are open and honest about the limitations of their work and the causes of these limitations and funders understand and are supportive of these goals.\nAcknowledgements\nWe thank Sebastian Funk for his feedback on an earlier version of this piece.\n\n\n\nAbbott, Sam, and Sebastian Funk. 2022. “EpiForecasts - Reflections on Two Years Estimating Effective Reproduction Numbers.” https://epiforecasts.io/posts/2022-03-25-rt-reflections/#an-attempt-to-design-a-useful-resource-for-situational-awareness.\n\n\n———. n.d. “Covid19.sgene.utla.rt: Recast to a Non-Linear Model.” https://github.com/epiforecasts/covid19.sgene.utla.rt/pull/42.\n\n\nAbbott, Sam, Joel Hellewell, James Munday, CMMID nCoV working group, and Sebastian Funk. 2020. “The Transmissibility of Novel Coronavirus in the Early Stages of the 2019-20 Outbreak in Wuhan: Exploring Initial Point-Source Exposure Sizes and Durations Using Scenario Analysis.” Wellcome Open Res 5 (February): 17. https://doi.org/10.12688/wellcomeopenres.15718.1.\n\n\nAbbott, Sam, Joel Hellewell, Katharine Sherratt, Katelyn Gostic, Joe Hickson, Hamada S. Badr, Michael DeWitt, Robin Thompson, EpiForecasts, and Sebastian Funk. 2020. EpiNow2: Estimate Real-Time Case Counts and Time-Varying Epidemiological Parameters. https://doi.org/10.5281/zenodo.3957489.\n\n\nAbbott, Sam, Joel Hellewell, Robin N. Thompson, Katharine Sherratt, Hamish P. Gibbs, Nikos I. Bosse, James D. Munday, et al. 2020a. “Estimating the time-varying reproduction number of SARS-CoV-2 using national and subnational case counts [version 1].” Wellcome Open Research 5 (December): 112. https://doi.org/10.12688/wellcomeopenres.16006.1.\n\n\n———, et al. 2020b. “Estimating the time-varying reproduction number of SARS-CoV-2 using national and subnational case counts [version 2].” Wellcome Open Research 5 (December): 112. https://doi.org/10.12688/wellcomeopenres.16006.2.\n\n\nAbbott, Sam, Adam J. Kucharski, and Sebastian Funk. 2021. “Estimating the Increase in Reproduction Number Associated with the Delta Variant Using Local Area Dynamics in England,” August.\n\n\nBrooks-Pollock, Ellen, Leon Danon, Thibaut Jombart, and Lorenzo Pellis. 2021. “Modelling That Shaped the Early COVID-19 Pandemic Response in the UK.” Philosophical Transactions of the Royal Society B: Biological Sciences 376 (1829): 20210001. https://doi.org/10.1098/rstb.2021.0001.\n\n\nCori, A, ZN Kamvar, J Stockwin, T Jombart, E Dahlqwist, R FitzJohn, and R Thompson. 2021. “EpiEstim v2.2-3: A tool to estimate time varying instantaneous reproduction number during epidemics.” GitHub Repository. https://github.com/mrc-ide/EpiEstim; GitHub.\n\n\nDavies, Nicholas G., Sam Abbott, Rosanna C. Barnard, Christopher I. Jarvis, Adam J. Kucharski, James D. Munday, Carl A. B. Pearson, et al. 2021. “Estimated transmissibility and impact of SARS-CoV-2 lineage B.1.1.7 in England.” Science 372 (6538): eabg3055. https://doi.org/10.1126/science.abg3055.\n\n\nFunk, S., S. Abbott, B. D. Atkins, M. Baguelin, J. K. Baillie, P. Birrell, J. Blake, et al. 2020. “Short-Term Forecasts to Inform the Response to the Covid-19 Epidemic in the UK.” https://doi.org/10.1101/2020.11.11.20220962.\n\n\nGostic, Katelyn M., Lauren McGough, Edward B. Baskerville, Sam Abbott, Keya Joshi, Christine Tedijanto, Rebecca Kahn, et al. 2020. “Practical Considerations for Measuring the Effective Reproductive Number, Rt.” PLOS Computational Biology 16 (12): e1008409. https://doi.org/10.1371/journal.pcbi.1008409.\n\n\nKucharski, Adam J., Sebastian Funk, and Rosalind M. Eggo. 2020. “The COVID-19 Response Illustrates That Traditional Academic Reward Structures and Metrics Do Not Reflect Crucial Contributions to Modern Science.” PLOS Biology 18 (10): e3000913. https://doi.org/10.1371/journal.pbio.3000913.\n\n\nMeyer, Sebastian, Leonhard Held, and Michael Höhle. 2017. “Spatio-Temporal Analysis of Epidemic Phenomena Using the R Package surveillance.” Journal of Statistical Software 77 (11): 1–55. https://doi.org/10.18637/jss.v077.i11.\n\n\nRECON-R Epidemics Consortium. n.d. “R Epidemics Consortium.” https://www.repidemicsconsortium.org/.\n\n\nSalmon, Maëlle, Dirk Schumacher, and Michael Höhle. 2016. “Monitoring Count Time Series in R: Aberration Detection in Public Health Surveillance.” Journal of Statistical Software 70 (10): 1–35. https://doi.org/10.18637/jss.v070.i10.\n\n\nSherratt, Katharine, Sam Abbott, Sophie R. Meakin, Joel Hellewell, James D. Munday, Nikos Bosse, null null, Mark Jit, and Sebastian Funk. 2021. “Exploring Surveillance Data Biases When Estimating the Reproduction Number: With Insights into Subpopulation Transmission of COVID-19 in England.” Philosophical Transactions of the Royal Society B: Biological Sciences 376 (1829): 20200283. https://doi.org/10.1098/rstb.2020.0283.\n\n\n“Welcome to the Epinowcast Community.” n.d. https://www.epinowcast.org/.\n\n\nWhitty, Christopher J. M. 2015. “What Makes an Academic Paper Useful for Health Policy?” BMC Medicine 13 (1): 301. https://doi.org/10.1186/s12916-015-0544-8.\n\n\nWhitty, Sir Chris, and Dame Angela McLean. n.d. “How Can Mathematical and Statistical Models Combine with Big Data to Improve Our Response to Pandemics?” https://www.lshtm.ac.uk/newsevents/events/how-can-mathematical-and-statistical-models-combine-big-data-improve-our-response.\n\n\nZelner, Jon, Julien Riou, Ruth Etzioni, and Andrew Gelman. 2021. “Accounting for Uncertainty During a Pandemic.” Patterns 2 (8). https://doi.org/10.1016/j.patter.2021.100310.\n\n\nslides are also available here: https://samabbott.co.uk/presentations/2022/how-can-governments-prepare-for-the-next-pandemic.pdf↩︎\n",
    "preview": {},
    "last_modified": "2025-03-18T11:09:01+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-19-applied-epi-application/",
    "title": "My application to be an instructor with Applied Epi",
    "description": "The cover letter I provided as part of my application to be an instructor with the Applied Epi organisation. Contains background on the motivation for my work, details of my prior experience, and thoughts on user based software development.",
    "author": [
      {
        "name": "Sam Abbott",
        "url": "https://samabbott.co.uk"
      }
    ],
    "date": "2022-08-19",
    "categories": [],
    "contents": "\n\nContents\nMotivation\nTechnical experience\nProfessional experience\nCode example\n\nHere I am sharing a public version of the cover letter I sent to Applied Epi as part of my application to be a course instructor. Applied Epi is a nonprofit that grew out of the community led development of the Epidemiologist R handbook. This resource provides crowd sourced reference material for practioners working in applied epidemiology and public health. The applied epidemiology handbook is in my view one of the best developments to come out of the response to the COVID-19 pandemic. Unlike traditional resources, tools, and training it takes a bottom up community focussed approach that delivers what is needed and not what stakeholders think is needed. Since the development of the handbook they have expanded to provide training and community developed tools based on user need. Both of these are sorely needed so this is an exciting development. You could say that I am a fan.\nI was very pleased to see tooling I developed as part of our COVID-19 response featured and found the comparison between other options informative for shaping my plans for future work. The key takeaway being that there is a large trade-off between optimal estimates and run-time. Users have different levels of technical expertise and computing resources and we need to build tools that provide gold standard estimates but that can also provide estimates of lower quality, with this quality reduction being quanitifable, when less time or resources are available. This was actually already a design focus of EpiNow2 (as much as anything written after several months of working 7 days a week for 18 hours a day can be said to be designed) but poor documentation coverage and potentially less than optimal design decisions led to this being hard for users to discover.\nAnother reason to make this application public is that I am currently exploring future career options that enable me to develop tools and methods for real-time infectious disease analysis with a community driven approach. Please let me know if you are part of an organisation that can support this or know of opportunities. I’ll be following up shortly with a 5 year research agenda that may provide useful additional detail.\nMotivation\nMy motivation to join the Applied Epi team stems from the focus of my research work which aims to improve the real-time analysis of infectious disease in both outbreak and routine surveillance settings. I developed one of the leading packages1 for real-time reproduction number estimation which is widely used by public health agencies and research groups responding to novel outbreaks and I am currently developing new tools and methods2. A core part of this work is to understand the needs of users as without being of practical use my work has little point3.\nCurrently to understand the needs of users I meet with practitioners from global health agencies and support them with their questions related to my areas of expertise. An example of this is case study which was developed with Sebastian Funk based on recent questions related to Monkeypox surveillance4. I also support research groups in person, for example SACEMA in South Africa5, when they need expertise at short notice. I would like to augment this by being involved with teaching practitioners in the early stages of their R journeys and ideally those learning about statistical modelling methodologies similar to those I develop. Aside from this selfish motivation, I am also a passionate advocate for open, democratised, science and see the work of Applied Epi as a great example of this.\nTechnical experience\nI have worked as a data scientist managing data extraction (SQL, R), data management, developing analyses using the tidyverse, and delivering routine reports in Rmarkdown and as shiny dashboards based on models I built. I also developed a series of Shiny apps and packages for accessing Tuberculosis data6, and democratising infectious disease models7. I am able to code in a range of styles with a preference for data.table in my own package development but believe it is key to adapt to a given project and user (as seen in my work). In recent years, I have shifted focus to more technical modelling work but have produced a series of real-time reports used as part of the UK government response to the COVID-19 pandemic and world-wide8. I have also developed pipelines using the targets package and GitHub actions that are now in use by others for nowcasting9 and forecasting10. My technical weaknesses are in a lack of recent SQL experience, in geospatial mapping, and as I have no experience with data collection tools.\nProfessional experience\nMy background is in academic research but I have been heavily involved in the UK response to the COVID-19 pandemic supplying real-time reports to government advisory bodies (SPI-M and SAGE) as well as forecasts to the CDC and ECDC forecasting hubs and nowcasts to the Germany nowcasting hub. I teach on the LSHTM modern methods in infectious disease modelling course and lead the software engineering session. I also taught on the Bristol University introduction to infectious disease modelling course and was responsible for developing the open source course content11. I co-supervise one PhD student and have managed (as PI) two software development projects by students12 which involved large mentoring components. I am currently supervising a Masters student using epinowcast to explore the impact of reporting structures for COVID-19 surveillance data on nowcast performance in the UK. I run the LSHTM CMMID software engineering slack channel and have a reputation for providing support as asked both for LSHTM students and those who contact me via Twitter13. My weaknesses in this area are that I have not worked for an applied epidemiology organisation and that in general my volume of teaching experience is somewhat limited.\nCode example\nAs previously noted, my code example14 was developed as a case study in response to questions related to Monkeypox surveillance. It showcases the use of the epinowcast package for symptom onset data that is reported with some delay, and some level of missingness, and where the analysis aim is to determine the underlying growth rate of the outbreak in real-time. The same repository also hosts a similar case study, largely written by Sebastian Funk, for the EpiNow2 package. The epinowcast case study also serves as a development road map as all the challenges with data of these kind that are noted will be supported as features in the near-term. This kind of user driven development style is one that I would like to make more use of as we move away from the response period of the pandemic and as I myself do less real-time response driven work and more methodological development. This aim is also behind the community driven leadership structure we have adopted for epinowcast with monthly meetings of developers, users, and methodology experts to decide the future of the package.\n\nEpiNow2: http://epiforecasts.io/EpiNow2↩︎\nFocussed on the epinowcast package: https://epiforecasts.io/epinowcast↩︎\nMore detail on my recent research is available in the following reflective pieces: https://samabbott.co.uk/posts/2022-04-11-a-very-biased-view-of-my-recent-research/, and https://epiforecasts.io/posts/2022-03-25-rt-reflections/index.html↩︎\nNowcasting case study: https://github.com/epiforecasts/nowcasting.example/blob/main/inst/reports/epinowcast.md↩︎\nSee some of SACEMA’s great work here: https://www.sacema.org/↩︎\ngetTBinR: https://github.com/seabbs/getTBinR↩︎\nidmodelr: https://github.com/seabbs/idmodelr↩︎\nSome example real-time reports are: https://github.com/epiforecasts/covid19.sgene.utla.rt, and https://github.com/epiforecasts/omicron-sgtf-forecast↩︎\nNowcasting targets pipeline: https://github.com/epiforecasts/eval-germany-sp-nowcasting↩︎\nForecasting GitHub Actions pipeline: https://github.com/epiforecasts/simplified-forecaster-evaluation↩︎\nBIDD infectious disease modelling course: https://bristolmathmodellers.github.io/biddmodellingcourse/↩︎\ncovidregionaldata: https://github.com/epiforecasts/covidregionaldata, scoringutils: https://github.com/epiforecasts/scoringutils↩︎\nFor example today: https://twitter.com/rafalpx/status/1560596503235665920↩︎\nNowcasting case study: https://github.com/epiforecasts/nowcasting.example/blob/main/inst/reports/epinowcast.md↩︎\n",
    "preview": {},
    "last_modified": "2025-03-18T11:09:00+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-11-a-very-biased-view-of-my-recent-research/",
    "title": "A very biased view of my recent research",
    "description": "A very biased overview of my recent research output as written for an internal promotion application. As such take everything with a large pinch of salt and replace all the I statements with We statements.",
    "author": [
      {
        "name": "Sam Abbott",
        "url": "https://samabbott.co.uk"
      }
    ],
    "date": "2022-04-11",
    "categories": [],
    "contents": "\n\nContents\nResearch overview\nHighlighted research\n\nA very biased overview of my recent research output as written for an internal promotion application. The brief was to summarise your research direction, your research output, steps taken to become an independent researcher, and then highlight 5 pieces of research output along with your contribution. As such take everything you read here with a large pinch of salt and replace all the I statements with We statements. However, if any of this sounds like fun to you then I am looking for collaborators and funding (ideally with few to no hoops).\nNote that this made me very uncomfortable to write and largely sharing as its hard to find samples of this kind of self-promotion. If you have feedback on how I can improve my writing with this aim in mind then please let me know.\nResearch overview\nMy main research interest lies in developing, evaluating, and applying methods for improving our understanding of infectious disease dynamics in real-time. I am committed to doing science in the open, and collaboratively, with the aim of producing useful and actionable output. Most of my recent work has been targeted towards the COVID-19 response but my underlying focus is pathogen agnostic sparse data settings.\nMy current main areas of work are developing and evaluating methods for nowcasting right truncated data, developing and evaluating methods to forecast and understand variant dynamics, reconstructing unobserved infections from a range of data sources (such as count data and prevalence measures), and developing methods for the estimation of the effective reproduction number, the growth rate, and generation interval distribution as well as use cases for these estimates and understanding their interactions. Longer term, I seek to widen these research interests to integrate other novel data sources and pathogen specific feature with a focus on providing the methodology and tools required to improve future epidemic and pandemic responses in both high and low resource settings. More generally, I aim to ensure these tools are of use for surveillance and research use and will continue my focus of building sustainable communities around my software tooling.\nSince the beginning of the pandemic I have been actively involved in the methodology and application of methods to estimate the effective reproduction number. This includes leading a project, in collaboration with the Met office, to estimate effective reproduction numbers across multiple geographies that had upwards of 500k users over two years and produced estimates that were used by multiple public health bodies (WHO, ECDC, UKHSA, etc) and numerous other research groups1. I supported this project by developing methodology to more accurately estimate the effective reproduction number2 and released open source tooling democratising access to this approach3. These methods continue to be commonly used by others and are often used as a baseline by those developing tools for surveillance4.\nAs previously noted (in an earlier section of the application on the impacts of COVID-19), much of my work in the last two years has been reactive and I have contributed numerous real-time reports on variants of concern mainly focussing on their transmission advantage, severity, and generation time. Some of these contributions are reflected in my publication record though a large part took the form of real-time rapid reports disseminated via twitter and offical channels (i.e SPI-M). During the recent emergance of the Omicron variant I supported researchers at SACEMA5 with their work on that variant. I have continued this support and will be travelling to South Africa in May for two weeks to present a seminar on my more recent tooling and to provide support to researchers there who are extending my earlier work on case fatality ratios estimated from population-level data.\nMy most recent project has been to improve nowcasting methodology (correction of right truncated counts). I have done this by developing a novel extension to previous nowcasting methodology6, releasing this in as a flexbile framework (in a space where such a tool has not been previously available)7, and developing a simple case study to facilitate use by others8. This work has then been contributed to the Germany nowcasting hub9, an international collaboration of researchers, which aims to provide daily nowcasts of German COVID-19 hospital admissions (one of the key metrics used by German policy makers). As an extension of this work I have released detailed daily evaluations of my own and others methodologies which have been used by myself and others to improve the quality of our nowcasts. I am now in the process of developing further collaborations on this work via a monthly open meeting and an active slack channel. This includes researchers at ETH Zurich and Stockholm University who I am currently supporting in order to allow them to add their innovations to my core framework. I am also co-supervising a masters student at the University of Warwick who is aiming to apply this tooling to COVID-19 data.\nI have also project managed (as PI) a number of recent pieces of work including the development of covidregionaldata an R package10 for accessing regional COVID-19 data authored by Joe Palmer11 (a PhD student from Royal Holloway) during his placement at LSHTM, and scoringutils an R package12 for scoring forecasts (with a corresponding article in preparation) authored by Nikos Bosse13 (PhD student at the school) for whom I also serve on his advisory committee.\nI am a co-investigator on two recent grant applications which have yet to return decisions. I am currently applying for a Chan Zuckerberg Initiative grant as part of the Essential Open Source Software for Science call to expand my suite of open source software and the community activities around them. In the course of the next year I aim to explore other funding streams such as the Wellcome Early Career Research Fellowship in order to expand the methodology behind my recent pandemic response work.\nOutside of my core academic work I routinely share my research, and summaries of others research on twitter (where I have over 2k+ followers).\nHighlighted research\nEstimating the time-varying reproduction number of SARS-CoV-2 using national and subnational case counts14\nThis work was widely used as a surveillance tool globally during the COVID-19 pandemic and also represents a substantial methodological advance in the field. It continues to be used as a baseline for assessing the robustness of other methods. I led the development of methodology and open-source software to estimate the time-varying effective reproduction number. I also led the development of the website front-end, data pipeline, and computational pipeline (in collaboration with the Met office) to provide estimates for over 1000 locations each day. The website where these estimates are presented has had over 500k unique users. Short-term forecasts from this pipeline have been submitted to SAGE, and the CDC and ECDC forecasting hubs. This work gave me insight into developing methods for real-time usage and their deployment at scale.\nEstimated transmissibility and impact of SARS-CoV-2 lineage B. 1.1. 7 in England15\nThis work was conducted by a larger collaboration under intense time pressure over Christmas 2021. It impacted UK policy directly and was used globally to inform government responses. The approach taken in this study was to triangulate key values using multiple methods. I was responsible for the development of one of these analyses as well as reviewing the implementation of the other approaches used. This work built on the reproduction number estimate pipeline I led the development of and we extended and repurposed this approach to estimate the transmissibility of the Delta variant and I subsequently further developed it to more robustly handle uncertainty16.\nEstimation of the test-to-test distribution as a proxy for generation interval distribution for the Omicron variant in England17\nThis work provided real-time evidence that the generation interval for the Omicron variant may be shorter than for the Delta variant. Evidence for this was key to policy decisions made during the Omicron wave and this evidence was some of the first available. I led the development of the novel semi-mechanistic method that linked Omicron growth rates with non-Omicron growth rates. I also led the development of the underlying software package18 used to produce these growth rate estimates and an earlier real time report tracking the time-varying transmission advantage of Omicron in England19.\nEvaluating Semi-Parametric Nowcasts of COVID-19 Hospital Admissions in Germany20\nDespite this work still being in progress I feel it captures the range of my research impact as it includes a core methodological development, work to democratise access to this development and previous developments, extensive and detailed evaluation (being drafted for peer review), offers utility to decision makers via the Germany nowcasting hub, support for other contributors for example researchers at ETH Zurich and at Stockholm University, and support for users for example the masters student at the University of Warwick who I will be co-supervising to use this tooling. By design it is modular and I expect this to be a fruitful area of research in the medium term. This project was undertaken fully independently with all aspects being planned and conducted without support from others.\nFeasibility of controlling COVID-19 outbreaks by isolation of cases and contacts21\nThis study was led by Joel Hellewell22 and was conducted in early 2020 when little was known about COVID-19. It provided timely and rigourous evidence23 that contact tracing alone would likely to insufficient for elimination and was widely used to inform policy. I supported all aspects of this analysis and led on the computational components which were crucial for its widespread reuse by other research groups.\n\nRead some reflections on this project here: https://samabbott.co.uk/posts/2022-03-25-rt-reflections/↩︎\nsome methodology details here: https://doi.org/10.12688/wellcomeopenres.16006.2↩︎\nEpiNow2: https://epiforecasts.io/EpiNow2↩︎\nFor example in this nice paper by Hay et al: https://doi.org/10.1126/science.abh0635↩︎\nSee their great work here: https://www.sacema.org/↩︎\nSee nowcast methodology details here: https://epiforecasts.io/epinowcast/articles/model.html↩︎\nNowcast package details here: https://epiforecasts.io/epinowcast↩︎\nNowcast case study details here: https://epiforecasts.io/eval-germany-sp-nowcasting/↩︎\nGermany nowcasting hub: https://covid19nowcasthub.de/↩︎\ncovidregionaldata: https://epiforecasts.io/covidregionaldata/↩︎\nCheck out Joe’s personal site here: https://joseph-palmer.github.io/↩︎\nscoringutills: https://epiforecasts.io/scoringutils/↩︎\nCheck Nikos’s blog out here: https://followtheargument.org/↩︎\nPreprint summarising this project here: https://doi.org/10.12688/wellcomeopenres.16006.2↩︎\nRead more about this here (though most of the details of my contribution are in the SI): https://doi.org/10.1126/science.abg305↩︎\nVia a very clever (if I do say so myself) use of brms: https://github.com/epiforecasts/covid19.sgene.utla.rt/blob/main/R/variant_rt.r↩︎\nAvailable as a preprint here: https://doi.org/10.1101/2022.01.08.22268920↩︎\nforecast.vocs: https://epiforecasts.io/forecast.vocs/↩︎\nSee this real-time report, and the code supporting it here: https://epiforecasts.io/omicron-sgtf-forecast/↩︎\nReal-time evaluation of nowcasting in Germany: https://epiforecasts.io/eval-germany-sp-nowcasting/, Nowcasting package: https://epiforecasts.io/epinowcast/↩︎\nSee here: https://doi.org/10.1016/S2214-109X(20)30074-7↩︎\nCheck out his blog here: https://jhellewell14.github.io/↩︎\nThere is some debate about this: https://doi.org/10.1016/S2214-109X(20)30219-9↩︎\n",
    "preview": {},
    "last_modified": "2025-03-18T11:09:00+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-25-rt-reflections/",
    "title": "Reflections on two years estimating effective reproduction numbers",
    "description": "Over the last two years we have estimated reproduction numbers daily for several thousand locations, presented these estimates as a curated data set and visualised them at epiforecasts.io/covid. In this post we reflect on this project, summarising its utility, its integration with other projects, unanticipated challenges, and finally whether we would do it again.",
    "author": [
      {
        "name": "Sam Abbott",
        "url": "https://samabbott.co.uk"
      },
      {
        "name": "Sebastian Funk",
        "url": "https://www.lshtm.ac.uk/aboutus/people/funk.sebastian"
      }
    ],
    "date": "2022-03-25",
    "categories": [],
    "contents": "\n\nContents\nAn attempt to design a useful resource for situational awareness\nAssessing utility\nFeeding into analysis pipelines\nUnanticipated challenges\nBack to 2020…would we do it again?\nAcknowledgments\n\nThis post was originally posted to epiforecasts.io and has been reposted here with consent of the authors.\n31 March, in just under a weeks time, will mark the last day we are producing global national and subnational Rt estimates and nowcasts at https://epiforecasts.io/covid/posts/global/ - more than 2 years after we published the first set of estimates. This is a good opportunity to reflect on what we have learned from this, what went well and what went wrong, and what we would aim to do better next time.\nAn attempt to design a useful resource for situational awareness\nWe started this with the aim to provide both decision makers and the general public with real-time information on how the epidemic was progressing, initially in China, then in a small subset of countries, and ultimately in different parts of the world across different geographic scales. We felt at the time that the most useful quantity to estimate was the time-varying effective reproduction number (Rt), as it characterises the exponential behaviour of transmission, captures some of the known epidemiology of infectious diseases, can be linked to when infections occur in a meaningful way, and can be used to quantify the scale of the effort required to turn over an epidemic.\nIn order to estimate this from the surveillance case data being published by countries around the world and collated by Johns Hopkins University, we had to develop methods (S. Abbott et al. 2020; Sam Abbott, Hellewell, Thompson, et al. 2020) that account for reporting artefacts and delays whilst taking into account emerging insights on the epidemiology of SARS-CoV-2, especially incubation and generation times. Our initial methodology was developed in the first few months of the pandemic (released in the EpiNow R package1 (Sam Abbott, Hellewell, Munday, et al. 2020)), with our updated approach (released in the EpiNow2 R package2 (Sam Abbott, Hellewell, Sherratt, et al. 2020)) being developed after discussions with colleagues on the limitations of our original implementation (Gostic et al. 2020). These packages ended up being the workhorses behind the web site and were used to provide daily updates of the estimates for almost all countries of the world, as well as several subnational geographies that we added over time, and in both our research group and in independent research groups for other projects.\n\n\n\nFigure 1: An example figure showing effective reproduction number estimates over time from a subset of countries.\n\n\n\nAssessing utility\nSo was it useful? Hard to tell. The website where we presented our estimates has had just over 500 thousand unique users since April 2020 with 1.2 million page views (with 800 thousand of these being for our US estimates, 120 thousand for our global estimates, and only 20 thousand for our methods - similar to the number of page view for our Swedish estimates). Usage has reduced over time but 8 thousand unique users still accessed the site in the last month.\nThe estimates themselves were processed regularly by various national international organisations such as the WHO, especially in the early parts of the epidemic. That said, we don’t know whether any of this gave any policy maker any useful information that helped them make better decisions or helped inform members of the public about their individual risk. Even if our estimates did help inform decision makers it is also not clear how much the evidence from our estimates improved on that available from other sources.\nIn its current and now final form, the web site still provides a somewhat unique resource for tracking the epidemic as we are not aware of another dashboard that collates both national and subnational Rt estimates from across the globe. That said, other websites like ourworldindata or the UK dashboard present more comprehensive raw surveillance information in a more interactive way, arguably rendering large parts of our public facing work obsolete.\nFeeding into analysis pipelines\nA perhaps more valuable contribution of our work was the publication of the estimates in numerical form that we provided on GitHub alongside the visualisations on the web site. These were used in numerous publications, including some by researchers that did not interact with us, making it hard for us to assess whether they were fully aware of the limitations underlying the estimates which are outlined in our companion paper (Sam Abbott, Hellewell, Thompson, et al. 2020).\nWe used the estimates ourselves in various downstream analyses, e.g. to monitor or local variation3, to investigate surveillance bias (Sherratt et al. 2021), to estimate transmission advantage of new variants (for Alpha (Davies et al. 2021), and for Delta (Sam Abbott et al. 2021)) or to estimate severity of infection4 over the course of the epidemic which was used in other work (Vöhringer et al. 2021) on the emergence and spread of the Alpha variant. Using routinely generated outputs of models as inputs to other models in such pipelines can be useful for multiple reasons. Primarily, this approach allows for rapid development when novel additions are needed in real-time with each step in the process dealing with some subset of the problem. It can also allow for access to novel methodology to be democratised in a way that is difficult with a complex model as the output can be used with potentially only a limited understanding of the implementation. Of course this can cause issues if the limitations of the method are poorly communicated. We have also observed our estimates being used by researchers from a range of backgrounds, with a range of tools at their disposal, that would be difficult to apply directly to the raw data prior to our domain knowledge based processing. Lastly, for complex problems chaining a series of models into a pipeline can reduce the computational burden and hence make analysis tractable when computational resources are limited. However, this approach may introduce bias to downstream results, potentially in ways that are difficult to diagnose or predict, as it can be difficult to fully incorporate uncertainty from all steps of the analysis pipeline into subsequent steps.\nUnanticipated challenges\nOur desire to provide a resource that was up-to-date, accurate and comprehensive posed some practical challenges. First of all, any epidemiological estimate is only ever as good as the data and method that is used to produce it. While our method was able to adjust for weekly effects, it hit its limits when e.g. reporting patterns changed or included unprecedented spikes on individual days. Additionally, estimating unobserved infections requires a good quantification of the incubation period, reporting delays, and generation time. These quantities can vary over time and by location, and are complex to estimate. For many regions and time periods specific estimates were not available and so we had to make additional assumptions to account for this. Most problematically, the quality, reliability and meaning of the data from different countries varies, and we did not have the capacity to manually curate or interpret the estimates we generated with respect to this underlying variation.\nThroughout the pandemic we attempted to link with local stakeholders to address some of these issues but this proved to be diffcult. We approached this in two ways. Firstly, by approaching other researchers and trying to link with them to jointly manage estimates for a particular geography related to their interests. We also released our backend data cleaning code as an open source R package, covidregionaldata5 (Palmer et al. 2021), for use by others. This allowed several datasets to be contributed by others and these could then be processed using our tooling to produce estimates with no further involvement from those maintaining access to the data itself. Of these approaches, the second approach where we maintained the front-end and estimation pipeline whilst making it easy to contribute data (and providing a useful service, i.e data cleaning, whilst doing so) was the more successful but ultimately this was a very challenging aspect of the project and we were largely unable to manage these collaborations effectively enough for this to lead to lasting collaborations. Because we did not have the capacity to manually inspect the data and estimates on a daily basis we sometimes published nonsensical estimates. Lack of capacity also meant that we struggled to explain how these came about and this caused some consternation6 at times amongst visitors to the web site. This was a particular issue for subnational estimates in countries without a unified surveillance framework, such as the USA, where reporting patterns and practices changed state by state throughout the pandemic.\nOther challenges were of more technical nature. Running a model daily on thousands of national and sub national data sets takes a huge amount of computation when an appropriately complex model is used. We benefited from a very generous grant from Microsoft AI for Health that enabled us to do this (and Microsoft, too, used our estimates in their own visualisations7), and it is the end of that grant that is prompting us to conclude this work. We also received a large amount of technical help from the UK Met Office in making our set up for distributed computing8 more sustainable which required a skillset that is difficult to acquire or sustain in academia.\nFinally, as for many research groups responding to the pandemic, we have been working significantly over capacity since January 2020. Producing and maintaining these estimates, and the infrastructure that supports them required a significant number of person hours, particularly prior to extensive automation and improvements in the robustness of processes, often out of hours over a prolonged period. This workload often led to more traditional academic work not being done and was likely exacerbated by a lack of the skills required to run complex models in production environments. A particular, and initially surprising, demand on time was responding to feedback from users which could often be complex, linked to public health policy in their region of interest, and sometimes abrupt, aggressive, or extremely negative.\nBack to 2020…would we do it again?\nWith all of this in mind we have been reflecting on whether we would do this again given the situation we were facing in early 2020 and, if yes, what we would do differently. Many of the challenges mentioned above were pretty much insurmountable, particularly around data quality and curation. Probably our most useful contributions from this work were in the UK where we knew the data and its limitations particularly well and interacted directly and frequently with policy makers.\nThat said, it would have been difficult to predict where the focus of our work would be when producing the initial estimates for China. In a perfect world we would have had methodologically robust, well evaluated, and production ready tools available to generate epidemiological estimates, and those trained to use them, in advance of a pandemic, such that these could be readily used by ourselves as well as teams everywhere in interaction with local policy makers and with a full understanding of the underlying data and its idiosyncrasies. It is great to see that there are now initiatives towards developing production ready tools for this purpose9, and we can only hope that these will be continued to put analytics of future epidemics on a more sustainable and reliable footing.\nHowever, we have not yet seen similar progress being made on initiatives to improve the methodological basis for these tools, evaluating their performance in different scenarios (especially low resource settings), and ensuring that there exists a pool of researchers with the right skills to make use of them in real-time and who can also communicate directly with local policy makers. Maintaining, and growing, a pool of skilled researchers able to deploy tools for situational awareness, which requires a different skill set to traditional research, is a particular hurdle as it is poorly supported by current funding models. Without sufficient support some of the progress that has been made developing researchers with these skill sets during the pandemic will almost certainly be lost.\nUltimately, initiatives of this scale, whether worthwhile or not, are likely to be conducted again and used to inform public health decisions if another epidemic/pandemic occurs. It is in the best interest of the public health community, and the public more generally, that these are as good as possible and not limited by the ability of those implementing them, the availability of computational and personal resources, weaknesses in surveillance systems, or weaknesses in the underlying methodology.\nAcknowledgments\nWe would like to thank Microsoft AI for Health for their generous computational support and the Met Office for providing technical assistance. We would also like to thank all those recording and aggregating surveillance data on which this work relies. Lastly, we would like to thank all co-authors and members of the CMMID COVID-19 working group for their contributions to this work.\n\n\n\nAbbott, Sam, Joel Hellewell, James Munday, Robin Thompson, and Sebastian Funk. 2020. “EpiNow: Estimate Realtime Case Counts and Time-Varying Epidemiological Parameters.” https://doi.org/10.5281/zenodo.3746392.\n\n\nAbbott, Sam, Joel Hellewell, Katharine Sherratt, Katelyn Gostic, Joe Hickson, Hamada S. Badr, Michael DeWitt, Robin Thompson, EpiForecasts, and Sebastian Funk. 2020. “EpiNow2: Estimate Real-Time Case Counts and Time-Varying Epidemiological Parameters.” https://doi.org/10.5281/zenodo.3957489.\n\n\nAbbott, Sam, Joel Hellewell, Robin N Thompson, Katharine Sherratt, Hamish P Gibbs, Nikos I Bosse, James D Munday, et al. 2020. “Estimating the Time-Varying Reproduction Number of SARS-CoV-2 Using National and Subnational Case Counts.” Wellcome Open Res. 5 (December): 112. https://doi.org/10.12688/wellcomeopenres.16006.2.\n\n\nAbbott, Sam, Adam J Kucharski, Sebastian Funk, and CMMID COVID-19 Working Group. 2021. “Estimating the Increase in Reproduction Number Associated with the Delta Variant Using Local Area Dynamics in England.” bioRxiv, December. https://doi.org/10.1101/2021.11.30.21267056.\n\n\nAbbott, S, J Hellewell, RN Thompson, K Sherratt, HP Gibbs, NI Bosse, JD Munday, et al. 2020. “Estimating the Time-Varying Reproduction Number of SARS-CoV-2 Using National and Subnational Case Counts (Version 1).” Wellcome Open Research 5 (112). https://doi.org/10.12688/wellcomeopenres.16006.1.\n\n\nDavies, Nicholas G, Sam Abbott, Rosanna C Barnard, Christopher I Jarvis, Adam J Kucharski, James D Munday, Carl A B Pearson, et al. 2021. “Estimated Transmissibility and Impact of SARS-CoV-2 Lineage b.1.1.7 in England.” Science 372 (6538). https://doi.org/10.1126/science.abg3055.\n\n\nGostic, Katelyn M, Lauren McGough, Edward B Baskerville, Sam Abbott, Keya Joshi, Christine Tedijanto, Rebecca Kahn, et al. 2020. “Practical Considerations for Measuring the Effective Reproductive Number, Rt.” PLoS Comput. Biol. 16 (12): e1008409. https://doi.org/10.1371/journal.pcbi.1008409.\n\n\nPalmer, Joseph, Katharine Sherratt, Richard Martin-Nielsen, Jonnie Bevan, Hamish Gibbs, Sebastian Funk, and Sam Abbott. 2021. “Covidregionaldata: Subnational Data for COVID-19 Epidemiology.” Journal of Open Source Software 6 (63): 3290. https://doi.org/10.21105/joss.03290.\n\n\nSherratt, Katharine, Sam Abbott, Sophie R Meakin, Joel Hellewell, James D Munday, Nikos Bosse, Null Null, Mark Jit, and Sebastian Funk. 2021. “Exploring Surveillance Data Biases When Estimating the Reproduction Number: With Insights into Subpopulation Transmission of COVID-19 in England.” Philos. Trans. R. Soc. Lond. B Biol. Sci. 376 (1829): 20200283. https://doi.org/10.1098/rstb.2020.0283.\n\n\nVöhringer, Harald S, Theo Sanderson, Matthew Sinnott, Nicola De Maio, Thuy Nguyen, Richard Goater, Frank Schwach, et al. 2021. “Genomic Reconstruction of the SARS-CoV-2 Epidemic in England.” Nature 600 (7889): 506–11. https://doi.org/10.1038/s41586-021-04069-y.\n\n\nDocumentation here: https://epiforecasts.io/EpiNow↩︎\nDocumentation here: https://epiforecasts.io/EpiNow2↩︎\nFor example in this simple report: https://github.com/epiforecasts/covid19_uk_local↩︎\nGitHub report and code to estimate reporting rates, infection-hospitalisation ratios, and infection-fatality ratios over time: https://github.com/epiforecasts/ons_severity_estimates↩︎\nDocumentation here: https://epiforecasts.io/covidregionaldata↩︎\nThis GitHub issue is a good example of the consternation caused by some of the more extreme data issues: https://github.com/epiforecasts/covid/issues/171↩︎\nMicrosoft AI COVID Dashboard which integrated our estimates: https://www.microsoft.com/en-us/ai/ai-for-health-covid-data↩︎\nSee their code contributions here: https://github.com/epiforecasts/covid-rt-estimates and here: https://github.com/epiforecasts/covid-rt-estimates-batch↩︎\nFor example the epiverse: https://data.org/initiatives/epiverse/↩︎\n",
    "preview": "posts/2022-03-25-rt-reflections/example-rt.png",
    "last_modified": "2025-03-18T11:09:00+00:00",
    "input_file": {},
    "preview_width": 3600,
    "preview_height": 3600
  },
  {
    "path": "posts/2021-11-10-blogdown-to-distill/",
    "title": "Moving from Blogdown to Distill",
    "description": "A short note to state the obvious point that I have switched from using Blogdown to Distill with postcards.",
    "author": [
      {
        "name": "Sam Abbott",
        "url": {}
      }
    ],
    "date": "2021-11-10",
    "categories": [],
    "contents": "\nA short note to state the obvious point that I have switched from using Blogdown to Distill with postcards. Full length blog posts and lay paper summaries will now be posted to this site whilst shorter notes and informal peer reviews will continue to be published to my notes site. Over the next few weeks I plan to build out the site a little more with short summaries of my research projects (both software and analysis) to aid discovery.\nDistill is a publication format for scientific and technical writing, native to the web and has become quite popular over the last few year. It is relatively simple to use and should hopefully be easier to maintain than my last blogdown site. At work I have been using Distill to power our COVID-19 surveillance site and whilst it isn’t the slickest website in the world it has been relatively easy to keep running over the last two years whilst server 500k+ users (though from the GitHub issues I could believe they all hated it).\nThis implementation draws heavily from the custom theme used by Miles Mcbain and from the Distillery. From a technical standpoint there is little that is particularly exciting here except potentially the use of child documents to integrate my GitHub about me page and presentation repository, and the staged deployment of the site using GitHub actions (building the site for all main branch pushes and for all PRs to the main branch and then only deploying to an orphaned gh-pages branch for main branch builds).\nMy old site and blog is still available at https://www.samabbott.co.uk/blogdown-site and I will be adding redirects to posts if and when people want to read them.\n\n\n\n",
    "preview": {},
    "last_modified": "2025-03-18T11:09:00+00:00",
    "input_file": {}
  }
]
